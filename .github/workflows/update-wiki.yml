name: Auto Extract Wiki Pages from Documentation

on:
  schedule:
    # Sunday 23:30 UTC = Monday 03:00 Iran
    - cron: "30 23 * * 0"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  publish:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout main repo
        uses: actions/checkout@v4
        with:
          path: main-repo

      - name: Clone target Wiki
        run: |
          git clone https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/intellsoft/time-lapse-software-with-cctv-playback-film.wiki.git wiki
          echo "‚úÖ Wiki cloned successfully"

      - name: Fetch source documentation page
        run: |
          curl -L "https://intellsoft.ir/docs/%d8%b1%d8%a7%d9%87%d9%86%d9%85%d8%a7%db%8c-%d9%86%d8%b1%d9%85-%d8%a7%d9%81%d8%b2%d8%a7%d8%b1-%d8%aa%d8%a8%d8%af%db%8c%d9%84-%d9%81%db%8c%d9%84%d9%85-%d8%af%d9%88%d8%b1%d8%a8%db%8c%d9%86-%d9%85%d8%af/" -o web-docs.html
          echo "‚úÖ Web page fetched"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4

      - name: Automatically detect article titles from web page
        run: |
          mkdir -p articles
          python3 - <<PYTHON
          import re
          from bs4 import BeautifulSoup

          with open("web-docs.html", encoding="utf-8") as f:
              soup = BeautifulSoup(f, "html.parser")

          # ÿ™ÿ¥ÿÆ€åÿµ ŸÖÿ≠ÿ™Ÿàÿß€å ÿßÿµŸÑ€å ÿµŸÅÿ≠Ÿá (ÿ®ÿ≥ÿ™Ÿá ÿ®Ÿá ÿ≥ÿßÿÆÿ™ÿßÿ± ŸÇÿßŸÑÿ® ÿ≥ÿß€åÿ™)
          content_div = (
              soup.find("div", class_="entry-content") or
              soup.find("article") or
              soup.find("main") or
              soup.body
          )

          titles = []
          seen = set()

          # ÿßÿ≥ÿ™ÿ±ÿßÿ™⁄ò€å €±: ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßÿ≤ ÿ™ŸÖÿßŸÖ ÿßŸÑŸÖÿßŸÜ‚ÄåŸáÿß€å€å ⁄©Ÿá ŸÖÿ≠ÿ™ŸÖŸÑ Ÿáÿ≥ÿ™ŸÜÿØ
          for elem in content_div.find_all(["p", "li", "h2", "h3", "h4"]):
              text = elem.get_text(strip=True)
              if not text:
                  continue

              # ŸÖÿπ€åÿßÿ±Ÿáÿß€å ÿ™ÿ¥ÿÆ€åÿµ ÿπŸÜŸàÿßŸÜ ŸÖŸÇÿßŸÑŸá:
              # 1. ÿ∑ŸàŸÑ ŸÖÿ™ŸÜ ÿ®€åŸÜ 10 ÿ™ÿß 150 ⁄©ÿßÿ±ÿß⁄©ÿ™ÿ± ÿ®ÿßÿ¥ÿØ (ÿπŸÜÿßŸà€åŸÜ ŸÖÿπŸÖŸàŸÑÿßŸã ⁄©Ÿàÿ™ÿßŸáŸÜÿØ)
              # 2. ÿ¥ÿßŸÖŸÑ ⁄©ŸÑŸÖÿßÿ™ ⁄©ŸÑ€åÿØ€å ŸÖÿ±ÿ™ÿ®ÿ∑ ÿ®ÿß ŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿ®ÿßÿ¥ÿØ (ÿßÿÆÿ™€åÿßÿ±€å ÿßŸÖÿß ÿØŸÇÿ™ ÿ±ÿß ÿ®ÿßŸÑÿß ŸÖ€å‚Äåÿ®ÿ±ÿØ)
              # 3. ŸÇÿ®ŸÑÿßŸã ÿ´ÿ®ÿ™ ŸÜÿ¥ÿØŸá ÿ®ÿßÿ¥ÿØ
              # 4. ÿ¥ÿßŸÖŸÑ ÿπÿ®ÿßÿ±ÿ™‚ÄåŸáÿß€å ÿ™⁄©ÿ±ÿßÿ±€å ÿπŸÖŸàŸÖ€å ŸÜÿ®ÿßÿ¥ÿØ (ŸÖÿ´ŸÑ "ÿßÿ¥ÿ™ÿ±ÿß⁄© ⁄Øÿ∞ÿßÿ±€å" ÿå "ÿ™ÿπÿØÿßÿØ ÿ®ÿßÿ≤ÿØ€åÿØ")
              if 10 < len(text) < 150:
                  # ŸÅ€åŸÑÿ™ÿ± ⁄©ÿ±ÿØŸÜ ÿ¨ŸÖŸÑÿßÿ™ ÿπŸÖŸàŸÖ€å
                  if any(word in text for word in ["ÿßÿ¥ÿ™ÿ±ÿß⁄©", "ÿ®ÿßÿ≤ÿØ€åÿØ", "ÿØ€åÿØ⁄ØÿßŸá", "ÿ®ÿ±⁄Üÿ≥ÿ®", "ÿØÿ≥ÿ™Ÿá"]):
                      continue
                  # ÿ≠ÿØÿßŸÇŸÑ €å⁄© ⁄©ÿßÿ±ÿß⁄©ÿ™ÿ± ŸÅÿßÿ±ÿ≥€å ÿØÿßÿ¥ÿ™Ÿá ÿ®ÿßÿ¥ÿØ
                  if not re.search(r'[\u0600-\u06FF]', text):
                      continue
                  # ÿ¨ŸÑŸà⁄Ø€åÿ±€å ÿßÿ≤ ÿ´ÿ®ÿ™ ÿ™⁄©ÿ±ÿßÿ±€å
                  if text not in seen:
                      seen.add(text)
                      titles.append(text)

          # ÿß⁄Øÿ± ÿ®ÿß ÿ±Ÿàÿ¥ ÿ®ÿßŸÑÿß ÿπŸÜŸàÿßŸÜ€å €åÿßŸÅÿ™ ŸÜÿ¥ÿØÿå ÿßÿ≥ÿ™ÿ±ÿßÿ™⁄ò€å ÿØŸàŸÖ:
          # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßÿ≤ ŸÑ€åÿ≥ÿ™‚ÄåŸáÿß€å ŸÖÿ¥ÿÆÿµ (ŸÖŸÖ⁄©ŸÜ ÿßÿ≥ÿ™ ÿØÿ± <ul> €åÿß <ol> ÿ®ÿßÿ¥ŸÜÿØ)
          if len(titles) == 0:
              for ul in content_div.find_all(["ul", "ol"]):
                  for li in ul.find_all("li"):
                      text = li.get_text(strip=True)
                      if 10 < len(text) < 150 and re.search(r'[\u0600-\u06FF]', text):
                          if text not in seen:
                              seen.add(text)
                              titles.append(text)

          # ÿ∞ÿÆ€åÿ±Ÿá ÿπŸÜÿßŸà€åŸÜ ÿØÿ± ŸÅÿß€åŸÑ
          with open("articles/titles.txt", "w", encoding="utf-8") as f:
              for title in titles:
                  f.write(title + "\n")

          print(f"‚úÖ {len(titles)} ÿπŸÜŸàÿßŸÜ ŸÖŸÇÿßŸÑŸá ÿ™ÿ¥ÿÆ€åÿµ ÿØÿßÿØŸá ÿ¥ÿØ:")
          for i, t in enumerate(titles, 1):
              print(f"  {i}. {t}")
          PYTHON

      - name: Generate Wiki pages from detected titles
        run: |
          cd wiki
          
          while IFS= read -r title; do
            # ÿ≥ÿßÿÆÿ™ ŸÜÿßŸÖ ŸÅÿß€åŸÑ ÿßÿ≤ ÿπŸÜŸàÿßŸÜ (ÿ≠ŸÅÿ∏ ÿ≠ÿ±ŸàŸÅ ŸÅÿßÿ±ÿ≥€åÿå ŸÅŸÇÿ∑ ÿ≠ÿ∞ŸÅ ⁄©ÿßÿ±ÿß⁄©ÿ™ÿ±Ÿáÿß€å ÿ∫€åÿ±ŸÖÿ¨ÿßÿ≤)
            slug=$(echo "$title" | sed 's/[\/:*?"<>|]/-/g' | tr ' ' '-' | tr -s '-')
            slug="${slug:0:80}"  # ŸÖÿ≠ÿØŸàÿØ€åÿ™ ÿ∑ŸàŸÑ
            file="$slug.md"
            
            echo "üîç Ÿæÿ±ÿØÿßÿ≤ÿ¥: $title -> $file"
            
            # ÿß⁄Øÿ± ÿµŸÅÿ≠Ÿá ÿßÿ≤ ŸÇÿ®ŸÑ Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØÿå ÿ®ÿßÿ≤ŸÜŸà€åÿ≥€å ŸÜŸÖ€å‚Äå⁄©ŸÜ€åŸÖ (ŸÖ€å‚Äåÿ™ŸàÿßŸÜ€åÿØ ÿ¥ÿ±ÿ∑ ÿ±ÿß ÿ®ÿ±ÿØÿßÿ±€åÿØ)
            if [ -f "$file" ]; then
              echo "‚è© ÿµŸÅÿ≠Ÿá ÿßÿ≤ ŸÇÿ®ŸÑ Ÿàÿ¨ŸàÿØ ÿØÿßÿ±ÿØ: $title"
              continue
            fi
            
            echo "üìù ÿß€åÿ¨ÿßÿØ ÿµŸÅÿ≠Ÿá ÿ¨ÿØ€åÿØ: $title"
            
            {
              echo "# $title"
              echo ""
              echo "ÿß€åŸÜ ÿµŸÅÿ≠Ÿá ÿßÿ≤ ŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿ±ÿ≥ŸÖ€å ŸÜÿ±ŸÖ‚ÄåÿßŸÅÿ≤ÿßÿ± **CCTV Timelapse Playback** ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿ¥ÿØŸá ÿßÿ≥ÿ™."
              echo ""
              echo "---"
              echo ""
              echo "üîó **ŸÖŸÜÿ®ÿπ:**"
              echo ""
              echo "[ŸÖÿ¥ÿßŸáÿØŸá ÿµŸÅÿ≠Ÿá ⁄©ÿßŸÖŸÑ ŸÖÿ≥ÿ™ŸÜÿØÿßÿ™](https://intellsoft.ir/docs/%d8%b1%d8%a7%d9%87%d9%86%d9%85%d8%a7%db%8c-%d9%86%d8%b1%d9%85-%d8%a7%d9%81%d8%b2%d8%a7%d8%b1-%d8%aa%d8%a8%d8%af%db%8c%d9%84-%d9%81%db%8c%d9%84%d9%85-%d8%af%d9%88%d8%b1%d8%a8%db%8c%d9%86-%d9%85%d8%af/?utm_source=github_wiki&utm_medium=wiki&utm_campaign=cctv_timelapse)"
              echo ""
              echo "---"
              echo ""
              echo "üîÑ ÿß€åŸÜ ÿµŸÅÿ≠Ÿá ÿ®Ÿá‚Äåÿ∑Ÿàÿ± ÿÆŸàÿØ⁄©ÿßÿ± ÿ™Ÿàÿ≥ÿ∑ GitHub Actions ÿßÿ≤ ÿ¢ÿÆÿ±€åŸÜ ŸÜÿ≥ÿÆŸá ŸÖÿ≥ÿ™ŸÜÿØÿßÿ™ ÿ®Ÿá‚Äåÿ±Ÿàÿ≤ÿ±ÿ≥ÿßŸÜ€å ŸÖ€å‚Äåÿ¥ŸàÿØ."
              echo ""
              echo "üìå **ÿ™Ÿàÿ≥ÿπŸá‚ÄåÿØŸáŸÜÿØŸá:** ÿπŸÑ€å ÿπÿ®ÿßÿ≥‚ÄåŸæŸàÿ±"
              
            } > "$file"
            
            echo "‚úÖ ÿµŸÅÿ≠Ÿá ÿ≥ÿßÿÆÿ™Ÿá ÿ¥ÿØ: $file"
            echo "------------------------"
            
          done < ../articles/titles.txt

      - name: Commit and push to Wiki
        run: |
          cd wiki
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add .
          if git diff --cached --quiet; then
            echo "üì≠ Ÿá€å⁄Ü ÿ™ÿ∫€å€åÿ±€å ÿ®ÿ±ÿß€å Commit Ÿàÿ¨ŸàÿØ ŸÜÿØÿßÿ±ÿØ."
          else
            git commit -m "Auto-sync documentation articles to Wiki"
            git push
            echo "üöÄ ÿ™ÿ∫€å€åÿ±ÿßÿ™ ÿ®ÿß ŸÖŸàŸÅŸÇ€åÿ™ ÿ®Ÿá Ÿà€å⁄©€å ÿßÿ±ÿ≥ÿßŸÑ ÿ¥ÿØ."
          fi
