name: Auto Extract Wiki Pages from Documentation (nav-link)

on:
  schedule:
    # Sunday 23:30 UTC = Monday 03:00 Iran
    - cron: "30 23 * * 0"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  publish:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout main repo
        uses: actions/checkout@v4
        with:
          path: main-repo

      - name: Clone target Wiki
        run: |
          git clone https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/intellsoft/time-lapse-software-with-cctv-playback-film.wiki.git wiki
          echo "âœ… Wiki cloned successfully"

      - name: Fetch main documentation page
        run: |
          curl -L "https://intellsoft.ir/docs/%d8%b1%d8%a7%d9%87%d9%86%d9%85%d8%a7%db%8c-%d9%86%d8%b1%d9%85-%d8%a7%d9%81%d8%b2%d8%a7%d8%b1-%d8%aa%d8%a8%d8%af%db%8c%d9%84-%d9%81%db%8c%d9%84%d9%85-%d8%af%d9%88%d8%b1%d8%a8%db%8c%d9%86-%d9%85%d8%af/" -o main.html
          echo "âœ… Main page fetched"

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install beautifulsoup4 requests

      - name: Extract nav-links and fetch articles
        run: |
          mkdir -p articles_raw
          python3 - <<PYTHON
          import os
          import requests
          from bs4 import BeautifulSoup
          import re

          # ============ Ø®ÙˆØ§Ù†Ø¯Ù† ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ ============
          with open("main.html", encoding="utf-8") as f:
              soup = BeautifulSoup(f, "html.parser")

          # ÛŒØ§ÙØªÙ† ØªÙ…Ø§Ù… Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ú©Ù„Ø§Ø³ nav-link
          nav_links = soup.find_all("a", class_="nav-link")
          print(f"âœ… {len(nav_links)} Ù„ÛŒÙ†Ú© nav-link ÛŒØ§ÙØª Ø´Ø¯.")

          # Ø§ÛŒØ¬Ø§Ø¯ ÙÙ‡Ø±Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‚Ø§Ù„Ø§Øª
          articles = []

          for idx, a in enumerate(nav_links, 1):
              href = a.get("href")
              title = a.get_text(strip=True)

              if not href or not title:
                  continue

              # Ø³Ø§Ø®Øª URL Ú©Ø§Ù…Ù„ (Ø§Ú¯Ø± Ù†Ø³Ø¨ÛŒ Ø¨ÙˆØ¯)
              if href.startswith("/"):
                  full_url = "https://intellsoft.ir" + href
              else:
                  full_url = href

              print(f"  {idx}. Ø¯Ø±ÛŒØ§ÙØª Ù…Ù‚Ø§Ù„Ù‡: {title} Ø§Ø² {full_url}")

              # Ø¯Ø±ÛŒØ§ÙØª ØµÙØ­Ù‡ Ù…Ù‚Ø§Ù„Ù‡
              try:
                  response = requests.get(full_url, timeout=15)
                  response.encoding = "utf-8"
                  article_soup = BeautifulSoup(response.text, "html.parser")
              except Exception as e:
                  print(f"     âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª: {e}")
                  continue

              # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¹Ù†ÙˆØ§Ù† Ù…Ù‚Ø§Ù„Ù‡ (Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ h1ØŒ Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ†ØµÙˆØ±Øª Ù‡Ù…Ø§Ù† title Ù„ÛŒÙ†Ú©)
              page_title = article_soup.find("h1")
              if page_title:
                  article_title = page_title.get_text(strip=True)
              else:
                  article_title = title

              # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡
              # Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¯Ø± <article> ÛŒØ§ <div class="entry-content"> Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯
              content_elem = (
                  article_soup.find("article") or
                  article_soup.find("div", class_="entry-content") or
                  article_soup.find("div", class_="content") or
                  article_soup.find("main")
              )

              if content_elem:
                  # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ (script, style, nav, footer)
                  for tag in content_elem.find_all(["script", "style", "nav", "footer", "header"]):
                      tag.decompose()

                  # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ†
                  article_text = content_elem.get_text("\n", strip=True)
                  # Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø·ÙˆÙ„ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
                  if len(article_text) > 50000:
                      article_text = article_text[:50000] + "â€¦"
              else:
                  article_text = "âš ï¸ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ù‚Ø§Ù„Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯."

              # Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
              articles.append({
                  "title": article_title,
                  "url": full_url,
                  "content": article_text
              })

              # Ø°Ø®ÛŒØ±Ù‡ ÛŒÚ© Ù†Ø³Ø®Ù‡ Ø®Ø§Ù… (Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯)
              with open(f"articles_raw/article_{idx}.txt", "w", encoding="utf-8") as f:
                  f.write(f"Ø¹Ù†ÙˆØ§Ù†: {article_title}\n")
                  f.write(f"URL: {full_url}\n")
                  f.write("-" * 50 + "\n")
                  f.write(article_text)

          # ============ Ø°Ø®ÛŒØ±Ù‡ ÙÙ‡Ø±Ø³Øª Ù†Ù‡Ø§ÛŒÛŒ ============
          with open("articles_list.txt", "w", encoding="utf-8") as f:
              for art in articles:
                  # ØªÙˆÙ„ÛŒØ¯ Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø§Ù…Ù† Ø§Ø² Ø¹Ù†ÙˆØ§Ù†
                  slug = art["title"].replace(" ", "-")
                  slug = re.sub(r'[\\/*?:"<>|]', '', slug)  # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ù…Ø¬Ø§Ø²
                  slug = slug[:80].strip("-")
                  f.write(f"{slug}\t{art['title']}\t{art['url']}\t{art['content']}\n")

          print(f"\nâœ… {len(articles)} Ù…Ù‚Ø§Ù„Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯.")
          PYTHON

      - name: Create Wiki pages from articles
        run: |
          cd wiki
          
          while IFS=$'\t' read -r slug title url content; do
            file="$slug.md"
            
            # Ø§Ú¯Ø± ØµÙØ­Ù‡ Ø§Ø² Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… (Ø¨Ø±Ø§ÛŒ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒØŒ Ø§ÛŒÙ† Ø´Ø±Ø· Ø±Ø§ Ø¨Ø±Ø¯Ø§Ø±ÛŒØ¯)
            if [ -f "$file" ]; then
              echo "â© ØµÙØ­Ù‡ Ø§Ø² Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯: $title"
              continue
            fi
            
            echo "ğŸ“ Ø§ÛŒØ¬Ø§Ø¯ ØµÙØ­Ù‡ ÙˆÛŒÚ©ÛŒ: $title"
            
            {
              echo "# $title"
              echo ""
              echo "Ø§ÛŒÙ† ØµÙØ­Ù‡ Ø§Ø² Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø±Ø³Ù…ÛŒ Ù†Ø±Ù…â€ŒØ§ÙØ²Ø§Ø± **CCTV Timelapse Playback** Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø³Øª."
              echo ""
              echo "---"
              echo ""
              echo "$content"
              echo ""
              echo "---"
              echo "ğŸ”— **Ù…Ù†Ø¨Ø¹ Ø§ØµÙ„ÛŒ:**"
              echo ""
              echo "ğŸ‘‰ [$url]($url?utm_source=github_wiki&utm_medium=wiki&utm_campaign=cctv_timelapse)"
              echo ""
              echo "ğŸ”„ Ø§ÛŒÙ† ØµÙØ­Ù‡ Ø¨Ù‡â€ŒØ·ÙˆØ± Ø®ÙˆØ¯Ú©Ø§Ø± ØªÙˆØ³Ø· GitHub Actions Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ù†Ø³Ø®Ù‡ Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯."
              echo ""
              echo "ğŸ“Œ **ØªÙˆØ³Ø¹Ù‡â€ŒØ¯Ù‡Ù†Ø¯Ù‡:** Ø¹Ù„ÛŒ Ø¹Ø¨Ø§Ø³â€ŒÙ¾ÙˆØ±"
              
            } > "$file"
            
            echo "âœ… ØµÙØ­Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯: $file"
            echo "------------------------"
            
          done < ../articles_list.txt

      - name: Commit and push to Wiki
        run: |
          cd wiki
          git config user.name "github-actions"
          git config user.email "actions@github.com"
          git add .
          if git diff --cached --quiet; then
            echo "ğŸ“­ Ù‡ÛŒÚ† ØªØºÛŒÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Commit ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯."
          else
            git commit -m "Auto-sync documentation articles to Wiki"
            git push
            echo "ğŸš€ ØªØºÛŒÛŒØ±Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ ÙˆÛŒÚ©ÛŒ Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯."
          fi
